{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price_to_float(data, data_dictionary):\n",
    "    \"\"\"\n",
    "    Function to convert string price columns into float price columns\n",
    "    :param data: type of data set\n",
    "    :param data_dictionary: dictionary with airbnb dataframes\n",
    "    :return: dictionary containing updated dataframes with float price columns\n",
    "    \"\"\"\n",
    "    string_columns = data_dictionary[data].select_dtypes(include=['object']).columns\n",
    "    price_columns = [col for col in string_columns if 'price' in col]\n",
    "    if len(price_columns) > 0:\n",
    "        for price in price_columns:\n",
    "            data_dictionary[data][price + '_float'] = data_dictionary[data][price]\\\n",
    "                .str\\\n",
    "                .replace('$', '')\\\n",
    "                .str\\\n",
    "                .replace(',', '')\\\n",
    "                .astype(float)\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(data, data_dictionary, date_column):\n",
    "    \"\"\"\n",
    "    Function to split date column into separate year, month, day columns\n",
    "    :param data: type of data set\n",
    "    :param data_dictionary: dictionary with airbnb dataframes\n",
    "    :param date_column: name of date column\n",
    "    :return: dictionary containing updated dataframes with new date columns\n",
    "    \"\"\"\n",
    "    if date_column in data_dictionary[data].select_dtypes(include=['object']).columns:\n",
    "        data_dictionary[data][[date_column +'_year'\n",
    "                               , date_column +'_month'\n",
    "                               , date_column +'_day']] = data_dictionary[data][date_column]\\\n",
    "            .str.split('-', 3, expand=True)\n",
    "    return data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(cities, data_sets, source_directory):\n",
    "    \"\"\"\n",
    "    Function to load three types of csv data sets for Boston and Seattle\n",
    "    :param city_type: list of city names\n",
    "    :param data_type: list of data sets\n",
    "    :param source_directory: location of the csv data sets\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    city_dictionary = {}\n",
    "    for city in cities:\n",
    "        data_dictionary = {}\n",
    "        for data in data_sets:\n",
    "            file_path = source_directory + '/' + data + '-' + city + '.csv'\n",
    "            # 1. read csv into dataframe\n",
    "            data_dictionary[data] = pd.read_csv(file_path)\n",
    "            # 2. convert string price columns into float columns\n",
    "            data_dictionary = convert_price_to_float(data, data_dictionary)\n",
    "            # 3. split date column into separate year, month, day column\n",
    "            data_dictionary = split_date(data, data_dictionary, 'date')\n",
    "            # 4. split host_since column into separate year, month, day column\n",
    "            data_dictionary = split_date(data, data_dictionary, 'host_since')\n",
    "        city_dictionary[city] = data_dictionary\n",
    "    return city_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dataframes(dictionary, data):\n",
    "    \"\"\"\n",
    "    Function to get combine datasets including data for each city\n",
    "    :param dictionary: complete data dictionary\n",
    "    :param data: particular data of interest\n",
    "    :return: dataframe including both cities\n",
    "    \"\"\"\n",
    "    seattle_df = dictionary['seattle'][data]\n",
    "    boston_df = dictionary['boston'][data]\n",
    "    seattle_df['boston'] = 0\n",
    "    boston_df['boston'] = 1\n",
    "    intersection_columns = set(seattle_df.columns).intersection(set(boston_df.columns))\n",
    "    df = pd.concat([seattle_df[intersection_columns], boston_df[intersection_columns]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df, missing_values_maximum):\n",
    "    \"\"\"\n",
    "    Function to check what columns have the most amount of missing data\n",
    "    :param df: dataframe with data to compare\n",
    "    :param missing_values_maximum: value spevifying the cutoff for missing data\n",
    "    :return: list of columns that have more missing data than specified amount\n",
    "    \"\"\"\n",
    "    seattle_data = df[df['boston'] == 0].isnull().mean().reset_index()\n",
    "    boston_data = df[df['boston'] == 1].isnull().mean().reset_index()\n",
    "    boston_data.columns = ['index', 'boston']\n",
    "    seattle_data.columns = ['index', 'seattle']\n",
    "    merged_data = seattle_data.merge(boston_data, how='inner', on='index')\n",
    "    merged_data = merged_data.sort_values(by=['boston', 'seattle'], ascending=False)\n",
    "    print(merged_data.head(60))\n",
    "    missing_data_list = merged_data[(merged_data['boston'] > missing_values_maximum) \n",
    "                               | (merged_data['seattle'] > missing_values_maximum)]['index'].tolist()\n",
    "    return missing_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_specified_columns(key, df):\n",
    "    \"\"\"\n",
    "    Function to drop columns that include a specified substring\n",
    "    :param key: substring to specify columns to delete \n",
    "    :param df: dataframe to remove columns from\n",
    "    :return: updated dataframe\n",
    "    \"\"\"\n",
    "    drop_columns = [col for col in df.columns if key in col.lower()]\n",
    "    if len(drop_columns) > 0:\n",
    "        df = df.drop(columns=drop_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_float_columns(df):\n",
    "    \"\"\"\n",
    "    Function to compare numeric series between Bostng and Seattle data\n",
    "    :param df: dataframe containing airbnb data for Seattle and Boston\n",
    "    \"\"\"\n",
    "    columns_to_check = df\\\n",
    "            .select_dtypes(include=['bool', 'int64', 'float64'])\\\n",
    "            .columns\n",
    "    for column in columns_to_check:\n",
    "        if 'id' not in column:\n",
    "            print('\\n Describe ' + column)\n",
    "            seattle_data = df[df['boston'] == 0][column].describe().reset_index()\n",
    "            seattle_data.columns = ['index', column + '_seattle']\n",
    "            boston_data = df[df['boston'] == 1][column].describe().reset_index()\n",
    "            boston_data.columns = ['index', column + '-boston']\n",
    "            merged_data = seattle_data.merge(boston_data, how='inner', on='index')\n",
    "            print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_specified_float_columns(df, compare_columns, agg_methods):\n",
    "    \"\"\"\n",
    "    Function to plot comparison of specified numeric series in boston and seattle data\n",
    "    :param df: dataframe containing airbnb data for Seattle and Boston\n",
    "    :param compare_columns: columns to compare\n",
    "    :param agg_methods: method to use for aggregation\n",
    "    \"\"\"\n",
    "    list_results = []\n",
    "    if not isinstance(agg_methods, list):\n",
    "        agg_methods = [agg_methods]\n",
    "    for method in agg_methods:\n",
    "        seattle_data = df[df['boston'] == 0][compare_columns].agg(method)\n",
    "        boston_data = df[df['boston'] == 1][compare_columns].agg(method)\n",
    "        if isinstance(seattle_data, pd.Series):\n",
    "            seattle_data = seattle_data.to_frame()\n",
    "            boston_data = boston_data.to_frame()\n",
    "        seattle_data.columns = [method + '-seattle']\n",
    "        boston_data.columns = [method + '-boston']\n",
    "        merged_data = seattle_data.merge(boston_data, how='inner', left_index=True, right_index=True)\n",
    "        list_results = list_results + [merged_data]\n",
    "    merged_data_all = pd.concat(list_results, axis=1, join='inner')\n",
    "    print(merged_data_all.head(len(compare_columns)))\n",
    "    ax = plt.gca()\n",
    "    merged_data_all.plot(kind='bar', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression(df, y_column, x_columns):\n",
    "    \"\"\"\n",
    "    Function to prepare training/testing data sets and run a linear regression\n",
    "    :param df: dataframe containing data to use regression on\n",
    "    :param y_column: RHS column name\n",
    "    :param x_columns: LHS columns names\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    X = df[x_columns]\n",
    "    y = df[y_column]\n",
    "\n",
    "    # Split data into training and test data, and fit a linear model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "    coeff_df = pd.DataFrame(np.transpose(lm_model.coef_), X.columns, columns=['Coefficient'])\\\n",
    "    .sort_values('Coefficient', ascending=False)\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    r2_test =  r2_score(y_test, y_test_preds)\n",
    "    results = dict()\n",
    "    results['coefficients'] = coeff_df\n",
    "    results['r2_test'] = r2_test\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_grouby_results(df, groupby_colunmns, agg_columns, method):\n",
    "    \"\"\"\n",
    "    Function to groupby Boston and Seattle data on specified columns and compare it\n",
    "    :param df: dataframe containing airbnb data for Seattle and Boston\n",
    "    :param groupby_colunmns: columns to use as a basis for aggregation\n",
    "    :param agg_columns: columns to aggregate and compare\n",
    "    :param agg_methods: method to use for aggregation\n",
    "    \"\"\"\n",
    "    seattle_data = df[df['boston'] == 0].groupby(groupby_colunmns)[agg_columns].agg(method)\n",
    "    boston_data = df[df['boston'] == 1].groupby(groupby_colunmns)[agg_columns].agg(method)\n",
    "    seattle_data.columns = [col + '-seattle' for col in agg_columns]\n",
    "    boston_data.columns =  [col + '-boston' for col in agg_columns]\n",
    "    merged_data = seattle_data.merge(boston_data, how='outer', left_index=True, right_index=True)\n",
    "    ax = plt.gca()\n",
    "    merged_data.plot(kind='bar', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
